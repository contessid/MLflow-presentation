{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5 â€” Model Serving\n",
    "\n",
    "Wrap the ARIMA model as a **pyfunc** for serving, then:\n",
    "1. Serve it locally with `mlflow models serve`\n",
    "2. Package it as a Docker image with `mlflow models build-docker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import mlflow\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from features import fourier_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5050\")\n",
    "mlflow.set_experiment(\"temperature-forecast-trial\")\n",
    "\n",
    "MODEL_NAME = \"temperature-forecast-trial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pyfunc wrapper for ARIMA\n",
    "\n",
    "The native statsmodels model can't be served directly via REST because its `predict()` API doesn't match what MLflow serving expects. We wrap it so that `predict()` accepts a DataFrame with an `n_steps` column, generates Fourier features for the forecast horizon, and returns forecasted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniele/miniconda3/envs/ml-gravio/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "class ARIMAWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Wraps a fitted statsmodels ARIMA for MLflow serving.\"\"\"\n",
    "\n",
    "    def __init__(self, ref_date, last_train_date):\n",
    "        self.ref_date = ref_date\n",
    "        self.last_train_date = last_train_date\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.model = mlflow.statsmodels.load_model(context.artifacts[\"arima_model\"])\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        from features import fourier_features\n",
    "\n",
    "        n_steps = int(model_input[\"n_steps\"].iloc[0])\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=self.last_train_date + pd.Timedelta(days=1),\n",
    "            periods=n_steps, freq=\"D\",\n",
    "        )\n",
    "        exog = fourier_features(forecast_dates, self.ref_date)\n",
    "        forecast = self.model.forecast(steps=n_steps, exog=exog)\n",
    "        return forecast.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, wrap, and register the pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted ARIMA(5, 1, 2) â€” AIC: 12649.09\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/jena_daily_temp.csv\", parse_dates=[\"Date Time\"], index_col=\"Date Time\")\n",
    "train = df.iloc[:-90]\n",
    "test = df.iloc[-90:]\n",
    "\n",
    "ref_date = df.index[0]\n",
    "exog_train = fourier_features(train.index, ref_date)\n",
    "\n",
    "# Fit the underlying ARIMA model\n",
    "order = (5, 1, 2)\n",
    "model = ARIMA(train[\"temperature\"], order=order, exog=exog_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 16:25:07 WARNING mlflow.pyfunc: Passing a Python object as `python_model` causes it to be serialized using CloudPickle, it requires exercising caution as Python object serialization mechanisms may execute arbitrary code during deserialization.Consider using a file path (str or Path) instead. See https://mlflow.org/docs/latest/ml/model/models-from-code/ for details.\n",
      "2026/02/26 16:25:07 INFO mlflow.pyfunc: Inferring model signature from input example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c85c4ebc4843b18ef071c0ba98359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bde737a91c47098072ee774584e397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'temperature-forecast-trial' already exists. Creating a new version of this model...\n",
      "2026/02/26 16:25:11 WARNING mlflow.tracking._model_registry.fluent: Run with id 413c68019e2b4ad2821520fbcb3775d0 has no artifacts at artifact path 'arima_model', registering model based on models:/m-13dd3c750b1d41db8981d762656b2ed9 instead\n",
      "2026/02/26 16:25:12 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: temperature-forecast-trial, version 3\n",
      "Created version '3' of model 'temperature-forecast-trial'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered pyfunc model v3 with @champion alias\n",
      "ðŸƒ View run pyfunc-serving at: http://localhost:5050/#/experiments/5/runs/413c68019e2b4ad2821520fbcb3775d0\n",
      "ðŸ§ª View experiment at: http://localhost:5050/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"pyfunc-serving\") as run:\n",
    "    # First log the statsmodels model as an internal artifact\n",
    "    arima_model_info = mlflow.statsmodels.log_model(results, name=\"statsmodels_arima\")\n",
    "\n",
    "    # Then log the pyfunc wrapper that references it\n",
    "    pyfunc_model_info = mlflow.pyfunc.log_model(\n",
    "        name=\"arima_model\",\n",
    "        python_model=ARIMAWrapper(ref_date=ref_date, last_train_date=train.index[-1]),\n",
    "        artifacts={\"arima_model\": arima_model_info.model_uri},\n",
    "        code_paths=[\"features.py\"],\n",
    "        input_example=pd.DataFrame({\"n_steps\": [30]}),\n",
    "    )\n",
    "\n",
    "    # Register the pyfunc model\n",
    "    mv = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/arima_model\",\n",
    "        name=MODEL_NAME,\n",
    "    )\n",
    "\n",
    "    # Set @champion alias\n",
    "    client = mlflow.MlflowClient()\n",
    "    client.set_registered_model_alias(MODEL_NAME, \"champion\", mv.version)\n",
    "\n",
    "    print(f\"Registered pyfunc model v{mv.version} with @champion alias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the pyfunc model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6ce648bd464b358eb5c04f6ad3454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-day forecast: [13.12626916874401, 13.15917272478445, 13.186079074311833, 12.973600147248318, 12.934278065035015, 12.728458773505682, 12.580103463176556]\n"
     ]
    }
   ],
   "source": [
    "loaded = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@champion\")\n",
    "\n",
    "result = loaded.predict(pd.DataFrame({\"n_steps\": [7]}))\n",
    "print(\"7-day forecast:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Serve with `mlflow models serve`\n",
    "\n",
    "Run this in a terminal:\n",
    "\n",
    "```bash\n",
    "MLFLOW_TRACKING_URI=http://localhost:5050 mlflow models serve \\\n",
    "  -m \"models:/temperature-forecast-simple@champion\" \\\n",
    "  -p 5001 --no-conda\n",
    "```\n",
    "\n",
    "> **Note:** We set `MLFLOW_TRACKING_URI` so the CLI can reach the dockerized MLflow server to download the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Forecast: {'predictions': [13.698623393911678, 14.213954983232066, 14.750731057335202, 14.93293845275739, 15.00506514684565, 15.059875746724265, 15.105628993375925, 15.152345467884409, 15.187955913472166, 15.213018386365713, 15.230453697613873, 15.242637644189825, 15.251652119773329, 15.258374527226302, 15.263364844718144, 15.267036421083583, 15.269713057560685, 15.271666581782412, 15.273095821462917, 15.274143972357361, 15.274913361038113, 15.275477654577916, 15.275891238025856, 15.2761942474753, 15.276416258083682, 15.276578963928538, 15.27669822250601, 15.276785637371207, 15.276849707569882, 15.276896664814437]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"n_steps\"],\n",
    "        \"data\": [[30]]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5001/invocations\",\n",
    "    json=payload,\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Forecast: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent curl command:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:5001/invocations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"dataframe_split\": {\"columns\": [\"n_steps\"], \"data\": [[30]]}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build a Docker image\n",
    "\n",
    "```bash\n",
    "MLFLOW_TRACKING_URI=http://localhost:5050 mlflow models build-docker \\\n",
    "  -m \"models:/temperature-forecast-simple@champion\" \\\n",
    "  -n temp-forecast-server\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "docker run -p 5001:8080 temp-forecast-server\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After starting the Docker container, test with the same request:\n",
    "response = requests.post(\n",
    "    \"http://localhost:5001/invocations\",\n",
    "    json=payload,\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Forecast: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**That's it!** We've gone from experiment tracking to a containerized model serving forecasts via REST API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gravio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
