{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5 — Model Serving\n",
    "\n",
    "Wrap the ARIMA model as a **pyfunc** for serving, then:\n",
    "1. Serve it locally with `mlflow models serve`\n",
    "2. Package it as a Docker image with `mlflow models build-docker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5050\")\n",
    "mlflow.set_experiment(\"temperature-forecast-simple\")\n",
    "\n",
    "MODEL_NAME = \"temperature-forecast-simple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pyfunc wrapper for ARIMA\n",
    "\n",
    "The native statsmodels model can't be served directly via REST because its `predict()` API doesn't match what MLflow serving expects. We wrap it so that `predict()` accepts a DataFrame with an `n_steps` column and returns forecasted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMAWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Wraps a fitted statsmodels ARIMA for MLflow serving.\"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import statsmodels.tsa.arima.model\n",
    "        self.model = mlflow.statsmodels.load_model(context.artifacts[\"arima_model\"])\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        n_steps = int(model_input[\"n_steps\"].iloc[0])\n",
    "        forecast = self.model.forecast(steps=n_steps)\n",
    "        return forecast.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, wrap, and register the pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/jena_daily_temp.csv\", parse_dates=[\"Date Time\"], index_col=\"Date Time\")\n",
    "train = df.iloc[:-90]\n",
    "test = df.iloc[-90:]\n",
    "\n",
    "# Fit the underlying ARIMA model\n",
    "order = (5, 1, 2)\n",
    "model = ARIMA(train[\"temperature\"], order=order)\n",
    "results = model.fit()\n",
    "\n",
    "print(f\"Fitted ARIMA{order} — AIC: {results.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with mlflow.start_run(run_name=\"pyfunc-serving\") as run:\n    # First log the statsmodels model as an internal artifact\n    arima_model_info = mlflow.statsmodels.log_model(results, name=\"statsmodels_arima\")\n\n    # Then log the pyfunc wrapper that references it\n    pyfunc_model_info = mlflow.pyfunc.log_model(\n        name=\"arima_model\",\n        python_model=ARIMAWrapper(),\n        artifacts={\"arima_model\": arima_model_info.model_uri},\n        input_example=pd.DataFrame({\"n_steps\": [30]}),\n    )\n\n    # Register the pyfunc model\n    mv = mlflow.register_model(\n        model_uri=f\"runs:/{run.info.run_id}/arima_model\",\n        name=MODEL_NAME,\n    )\n\n    # Set @champion alias\n    client = mlflow.MlflowClient()\n    client.set_registered_model_alias(MODEL_NAME, \"champion\", mv.version)\n\n    print(f\"Registered pyfunc model v{mv.version} with @champion alias\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the pyfunc model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@champion\")\n",
    "\n",
    "result = loaded.predict(pd.DataFrame({\"n_steps\": [7]}))\n",
    "print(\"7-day forecast:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 1: Serve with `mlflow models serve`\n\nRun this in a terminal:\n\n```bash\nMLFLOW_TRACKING_URI=http://localhost:5050 mlflow models serve \\\n  -m \"models:/temperature-forecast-simple@champion\" \\\n  -p 5001 --no-conda\n```\n\n> **Note:** We set `MLFLOW_TRACKING_URI` so the CLI can reach the dockerized MLflow server to download the model artifacts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"n_steps\"],\n",
    "        \"data\": [[30]]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5001/invocations\",\n",
    "    json=payload,\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Forecast: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent curl command:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:5001/invocations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"dataframe_split\": {\"columns\": [\"n_steps\"], \"data\": [[30]]}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Build a Docker image\n\n```bash\nMLFLOW_TRACKING_URI=http://localhost:5050 mlflow models build-docker \\\n  -m \"models:/temperature-forecast-simple@champion\" \\\n  -n temp-forecast-server\n```\n\nThen run:\n\n```bash\ndocker run -p 5001:8080 temp-forecast-server\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After starting the Docker container, test with the same request:\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5001/invocations\",\n",
    "    json=payload,\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Forecast: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**That's it!** We've gone from experiment tracking to a containerized model serving forecasts via REST API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gravio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}