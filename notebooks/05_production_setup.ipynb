{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus ‚Äî Production Setup\n",
    "\n",
    "Move from local docker-compose to production:\n",
    "- **Backend store:** Supabase PostgreSQL\n",
    "- **Artifact store:** Azure Blob Storage\n",
    "\n",
    "All secrets are loaded from a `.env` file (see `.env.example` for the template)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supabase PostgreSQL (backend store)\n",
    "\n",
    "Connection string (use **session pooler**, port 5432):\n",
    "\n",
    "```\n",
    "postgresql+psycopg2://postgres.<REF>:<PASS>@aws-0-<REGION>.pooler.supabase.com:5432/postgres?sslmode=require\n",
    "```\n",
    "\n",
    "> **Do NOT use port 6543** (transaction pooler) ‚Äî it breaks prepared statements that SQLAlchemy/MLflow relies on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Azure Blob Storage (artifact store)\n",
    "\n",
    "Artifact root format:\n",
    "```\n",
    "wasbs://mlflow-artifacts@<account>.blob.core.windows.net/\n",
    "```\n",
    "\n",
    "Requires: `pip install azure-storage-blob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load configuration from `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend store: postgresql+psycopg2://postgres:VN4AuFGT2...\n",
      "Artifact root: wasbs://mlflow-artifacts@stmlflowfd2025.blob.core.windows.net/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "backend_store_uri = os.environ[\"MLFLOW_BACKEND_STORE_URI\"]\n",
    "artifact_root = os.environ[\"MLFLOW_ARTIFACT_ROOT\"]\n",
    "\n",
    "# AZURE_STORAGE_CONNECTION_STRING is read automatically by azure-storage-blob\n",
    "print(f\"Backend store: {backend_store_uri[:40]}...\")\n",
    "print(f\"Artifact root: {artifact_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launch the MLflow server\n",
    "\n",
    "From the repo root:\n",
    "\n",
    "```bash\n",
    "# Load env vars and start the server\n",
    "set -a && source .env && set +a\n",
    "\n",
    "mlflow server \\\n",
    "  --backend-store-uri \"$MLFLOW_BACKEND_STORE_URI\" \\\n",
    "  --default-artifact-root \"$MLFLOW_ARTIFACT_ROOT\" \\\n",
    "  --host 127.0.0.1 --port 5050\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to http://localhost:5050\n",
      "Found 1 experiment(s):\n",
      "  - Default\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "PRODUCTION_URI = \"http://localhost:5050\"\n",
    "\n",
    "mlflow.set_tracking_uri(PRODUCTION_URI)\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "experiments = client.search_experiments()\n",
    "print(f\"Connected to {PRODUCTION_URI}\")\n",
    "print(f\"Found {len(experiments)} experiment(s):\")\n",
    "for exp in experiments:\n",
    "    print(f\"  - {exp.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/23 23:19:11 INFO mlflow.tracking.fluent: Experiment with name 'production-smoke-test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run smoke-test at: http://localhost:5050/#/experiments/1/runs/0f8f4c6093964200a6a80cf0802e055c\n",
      "üß™ View experiment at: http://localhost:5050/#/experiments/1\n",
      "Smoke test passed ‚Äî backend and artifact store working.\n"
     ]
    }
   ],
   "source": [
    "# Smoke test: log a dummy run to verify backend + artifact store\n",
    "mlflow.set_experiment(\"production-smoke-test\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"smoke-test\"):\n",
    "    mlflow.log_param(\"test\", \"connection\")\n",
    "    mlflow.log_metric(\"dummy\", 42.0)\n",
    "\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False) as f:\n",
    "        f.write(\"artifact round-trip test\")\n",
    "        tmp_path = f.name\n",
    "    mlflow.log_artifact(tmp_path)\n",
    "    os.unlink(tmp_path)\n",
    "\n",
    "print(\"Smoke test passed ‚Äî backend and artifact store working.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Browse artifacts in Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts in Azure Blob Storage:\n",
      "\n",
      "  1/0f8f4c6093964200a6a80cf0802e055c/artifacts/tmpvxi0gc1t.txt  (24 bytes)\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "connection_string = os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"]\n",
    "container = ContainerClient.from_connection_string(connection_string, \"mlflow-artifacts\")\n",
    "\n",
    "print(\"Artifacts in Azure Blob Storage:\\n\")\n",
    "for blob in container.list_blobs():\n",
    "    print(f\"  {blob.name}  ({blob.size} bytes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
